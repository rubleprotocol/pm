# Core Devs Community Call 20
### Meeting Date/Time: July 18, 2024, 7:00-8:00 UTC
### Meeting Duration: 60 Mins
### [GitHub Agenda Page](https://github.com/tronprotocol/pm/issues/96)
### Agenda
* [Improve Unit Tests Code Coverage to 70%](https://github.com/tronprotocol/java-tron/issues/5434)
* [More detailed results of the execution of the CANCELALLUNFREEZEV2(0xdc) opcode](https://github.com/tronprotocol/java-tron/issues/5530)

### Details
* Jake

  OK, let’s roll. Today we have the Core Devs Community Call 20th, and there are two issues need to be discussed. The first one is shared by Daniel.

* Daniel

  Sure, alright. Since last October, we have experienced several updates on the unit test coverage rate, and before that, Kayle was in charge of it. Here are a few records of the previous updates on the unit test coverage rate. Regarding the details of the unit test coverage rate for each module, currently, I see that the latest unit test coverage rate is 66.83%, which is a few percentage points short of 70%. According to the unit test coverage rate of each module, the coverage rate of the Framework part is relatively low, with Actuator at 75%, Chainbase at 77%, Common at 61%, and Consensus at 63.75%. My idea is to increase the coverage rate of the modules that are below 70%, and the team should try to increase the coverage rate as close to 70% as possible, as the coverage rate of each module is too low.

  Next, we can specifically check which modules have a lower coverage rate, which is under the framework directory. There are several sub-modules under Framework, including Common, Core, Keystore, and Program, and the coverage rates of these are all very low and seriously insufficient. Regarding Program, my idea is to allocate tasks among several active teams in the community to try to increase the unit test coverage rate as much as possible, whether it is to improve other modules or mainly to improve the framework module, we should try to increase the overall coverage rate to reach a goal of 70%.

  Ray, are you there?

* Ray

  Yeah.

* David

  We cannot solely rely on the coverage rate here; we also need to consider the number of lines of code in each module. Otherwise, it will be difficult to allocate the work. The specific workload is different.

* Daniel

  I understand that the number of code lines in each module varies greatly. For example, if the coverage rate of the framework module can reach 71%, it is estimated that the overall coverage rate can also reach 70%.

* Ray

  Should we reconfirm the content details and the workload in each module? If time is limited, it is best to focus our energy on the part with the highest priority and increase the proportion. This involves a more detailed task division.

* David

  This workload may be very large. For example, there are more than 20,000 lines of code in Core. If it is allocated to developers in the community, the workload will vary greatly.

* Daniel

  I just proposed to increase the coverage rate. I did not directly allocate the work, and it still needs to be discussed.

* Ray

  Although I have not looked at it carefully, I understand that some unit test coverage rates are very low, possibly because they are not very important. So, should we sort out a priority for each module? We can first divide the key and easy-to-do parts, and then see which module is suitable for which team to do or to be allocated evenly.

* Jake

  Then, Daniel, could you supplement this in the issue later, write in more detail about the approximate workload of each module, and then see the comments and reactions from the community?

* Daniel

  I think this is fine.

* Jake

  Ray, what do you think?

* Ray

  I think this form is also good. Because the workload is huge, it still needs to be discussed more widely in the community. It is a good idea to supplement it in the issue and let a wider range of developers discuss it.

* Jake

  OK, let's temporarily settle this issue like this. Does anyone else have any other questions?

* Jake

  OK, then let's move on to the next topic. Regarding the opcode, was it submitted by you, David?

* David

  Yes, I will share the screen.

  The purpose of this function is as follows. In the system contract, we all know that there is an interface called `cancelallunfreezeV2`, which can cancel all the user's queued unfreeze or unstake operations. In the VM, there is also an instruction that implements a similar function to the system interface. The result of the internal transaction generated by this instruction, does not include the canceled quantity of each resource, only the total canceled amount. This is somewhat different from the current implementation of the system and interface. At the same time, for some statistical services, they may need to know which resources will be canceled in the instruction-level cancellation operation. This is the main purpose of this issue.

  There are two implementation methods. Plan A is to add a new internal transaction type for each resource. We see that there are three types of bandwidth, energy, and tron power. Plan B implementation method is to write the specific quantity of each resource in the note field of the internal transaction, which is equivalent to using the note field to record the cancellation quantity of the three resources separately.

  There have been some small discussions on this issue in the early stage, mainly focusing on the implementation method. I personally mentioned that some community members are more inclined to use Plan B. Plan A requires adding three transaction types, so for the back-end service and statistical service, they may need to add additional code to parse these three.

  Regarding the purpose and implementation plan of this function, does anyone have any opinions?

* Daniel

  I see that a field has also been added in the system contract to record the canceled amount of these three types of resources separately. Then we can align with the system contract.

* David

  Yes, that's right. The recording format uses a JSON format to record.

* Ray

  Where is it recorded?

* David

  In the extended field, `string extra`.

  There is another issue that needs to be discussed. Does this function require a hard fork? The current implementation plan adopts a switch mode, and this switch can be configured to control whether detailed information about the cancellation instruction is added to the extended field of the transaction.

* Ray

  I want to confirm a question. Where is it placed in the internal transaction?

* David

  Inside the `transaction info`.

* Ray

  If the user is also monitoring events at the same time, will this field appear in the event report?

* David

  If the switch is turned on, it will appear. If it is obtained, if he is monitoring the entire transaction's transaction info, this field will appear.

* David

  This approach is similar to the previous one. When Stake 2.0 was launched, some internal transactions related to Stake 2.0 instructions were added. At that time, a switch design like this was used. This is because there is a hard fork, and the increase in internal transactions is not controlled by the hard fork. The switch is closed by default, and for users who need to track and record the cancel instruction and which type of resources unstaked are canceled, they can choose to turn on the switch.

* Ray

  From the perspective of monitoring contract events, can it only add a log to the contract?

* David

  Monitoring events needs to be in the implementation code, usually written in Solidity. The event logic has nothing to do with the instruction level, but it needs to be considered in the contract logic.

* Ray

  It is necessary to add the log output of the instruction execution at the instruction level.

* David

  Yes, specifically for this instruction. This instruction originally generated an internal transaction with only a total cancel amount and no specific type. Now, a switch is added to complete this. When the switch is turned on, the canceled resource type will be shown in the internal transaction.

* Ray

  If it is Plan A, there may be compatibility issues. Is this a concern?

* David

  Plan A is more like the processing method of new internal transactions when stake 2.0 was launched. All the instructions related to stake 2.0 added in the VM will add one or more corresponding internal transactions, and the switch mentioned earlier was used for control at that time.

* Ray

  I am mainly not clear about the specific advantages and disadvantages of Plan A or B. Do both require a hard fork?

* David

  The implementation of Plan B does not require a hard fork. It just uses a switch to control whether to output detailed information about the canceled resource types.

* Ray

  Then Plan A requires?

* David

  In fact, Plan A may not be required either. It is only necessary to clarify the storage location when storing the specific resource quantity. Three internal transactions are added, which is equivalent to storing the canceled amount of each resource. For the cancellation instruction, if Plan A is adopted, it may generate 1 to 4 internal transactions. For B, it is still the original situation, generating one internal transaction.

* Ray

  Will B affect the business parsing?

* David

  No, the extended field is not new. The voting instruction will use this field to store voting information.

* Ray

  Will there be a situation where both the voting information and the details of the canceled resources need to be stored in one instruction?

* David

  No, voting and canceling are two instructions and will not reuse the same internal transaction. For some active nodes, transaction info will participate in the calculation of the state, and I am not sure about this.

* Ray

  Me neither, and it needs to be confirmed.

* David

  If both nodes are active nodes, and one turns on this function while the other does not, it will definitely affect the state calculation.

* Ray

  This part should not be involved in the calculation because theoretically, the `info` itself can also be switched, and the whole is controllable.

* David

  What's everyone's opinion? Should it be made into a switch or a hard fork? Is a hard fork making a mountain out of a molehill?

* Ray

  From the current situation, I think a switch is more reasonable. However, if it is from the perspective that the transaction info needs to have a state info root and needs to be extended to a consensus, then a hard fork is still better.

* David

  In fact, there is already a switch for internal transactions. Even if we want to do the info root in the future, internal transactions will not participate in the calculation of the root.

* Ray

  Oh, that's right.

* David

  For the consensus, internal transactions need to be closed. When calculating the root, only the common part that is not controlled by the switch should be calculated.

* Ray

  Yes, that's right.

* David

  What do you think about the necessity of this function?

* Ray

  I think it is still necessary from the perspective of the development of the ecosystem.

* David

  There must be many upgrading requirements for Fullnode. That is to say, from the perspective of the community, is the priority of this function high?

* Ray

  I think if such a function can be provided and compatibility can be maintained, it may be a reasonable idea. If this function is made into a switch, the development difficulty and cycle will be just fine, no problem.

* David

  Does anyone else have any opinions?

* Jake

  If not, then that's it for today. Thank you all for your time. Goodbye!

  
  
  
  
### Attendance
* Ray
* David
* Andy
* Daniel
* Super
* Murphy
* Jake